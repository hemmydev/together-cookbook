{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa2dc6b",
   "metadata": {},
   "source": [
    "# Model Comparison on Summarization Tasks\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/togethercomputer/together-cookbook/blob/main/Evals/Compare_Evals.ipynb)\n",
    "\n",
    "<img src=\"../images/compare_eval.png\" width=\"750\">\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to compare two language models on a summarization task using the Together AI Evaluations API. We'll:\n",
    "\n",
    "1. Load the SummEval dataset containing news articles to summarize\n",
    "2. Configure two models for comparison\n",
    "3. Use a judge model to evaluate which summaries are better\n",
    "4. Analyze the head-to-head comparison results\n",
    "\n",
    "You can also find out more about the Evaluations API in the [docs](https://docs.together.ai/docs/ai-evaluations)!\n",
    "\n",
    "The full list of supported models can be found [here](https://docs.together.ai/docs/evaluations-supported-models).\n",
    "\n",
    "\n",
    "**Concepts Covered:**\n",
    "- **LLM-as-a-Judge**: Using a language model to evaluate and compare outputs from other models\n",
    "- **Compare Evaluation**: Head-to-head comparison between two models to determine which performs better\n",
    "- **Summarization Evaluation**: Assessing summary quality across multiple criteria (accuracy, completeness, clarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae764b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup and installation\n",
    "!pip install -qU together datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daef6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import together\n",
    "\n",
    "together_client = together.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ed3ca8",
   "metadata": {},
   "source": [
    "#### Let's imagine that we want to compare the performance of our models on a task—in this case, summarization. We will use the SummEval dataset, which contains media articles that we will summarize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d307fe1",
   "metadata": {},
   "source": [
    "## 📊 Understanding the SummEval Dataset\n",
    "\n",
    "The SummEval dataset contains news articles paired with both human and machine-generated summaries, along with quality ratings across multiple dimensions like relevance, coherence, fluency, and consistency.\n",
    "\n",
    "**Dataset Structure:**\n",
    "- `text`: The original news article to be summarized\n",
    "- `machine_summaries`: Various automated summaries\n",
    "- `human_summaries`: Human-written reference summaries\n",
    "- Quality ratings across multiple evaluation criteria\n",
    "\n",
    "For our evaluation, we'll focus on the original articles and generate new summaries using our target models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58e13d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['machine_summaries', 'human_summaries', 'relevance', 'coherence', 'fluency', 'consistency', 'text', 'id'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "summ_eval = load_dataset(\"mteb/summeval\")\n",
    "\n",
    "summ_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8e88c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machine_summaries</th>\n",
       "      <th>human_summaries</th>\n",
       "      <th>relevance</th>\n",
       "      <th>coherence</th>\n",
       "      <th>fluency</th>\n",
       "      <th>consistency</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[donald sterling , nba team last year . sterli...</td>\n",
       "      <td>[V. Stiviano must pay back $2.6 million in gif...</td>\n",
       "      <td>[1.6666666666666667, 1.6666666666666667, 2.333...</td>\n",
       "      <td>[1.3333333333333333, 3.0, 1.0, 2.6666666666666...</td>\n",
       "      <td>[1.0, 4.666666666666667, 4.333333333333333, 4....</td>\n",
       "      <td>[1.0, 2.3333333333333335, 4.666666666666667, 5...</td>\n",
       "      <td>(CNN)Donald Sterling's racist remarks cost him...</td>\n",
       "      <td>cnn-test-404f859482d47c127868964a9a39d1a7645dd2e9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[north pacific gray whale has earned a spot in...</td>\n",
       "      <td>[The whale, Varvara, swam a round trip from Ru...</td>\n",
       "      <td>[2.3333333333333335, 4.666666666666667, 3.6666...</td>\n",
       "      <td>[1.3333333333333333, 4.666666666666667, 3.6666...</td>\n",
       "      <td>[1.0, 5.0, 4.666666666666667, 3.66666666666666...</td>\n",
       "      <td>[1.3333333333333333, 5.0, 5.0, 4.3333333333333...</td>\n",
       "      <td>(CNN)A North Pacific gray whale has earned a s...</td>\n",
       "      <td>cnn-test-4761dc6d8bdf56b9ada97104113dd1bcf4aed3f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[russian fighter jet intercepted a u.s. reconn...</td>\n",
       "      <td>[The incident occurred on April 7 north of Pol...</td>\n",
       "      <td>[4.0, 4.0, 4.0, 3.3333333333333335, 3.33333333...</td>\n",
       "      <td>[3.3333333333333335, 4.333333333333333, 1.6666...</td>\n",
       "      <td>[3.6666666666666665, 4.333333333333333, 5.0, 4...</td>\n",
       "      <td>[5.0, 5.0, 4.666666666666667, 5.0, 5.0, 5.0, 5...</td>\n",
       "      <td>(CNN)After a Russian fighter jet intercepted a...</td>\n",
       "      <td>cnn-test-5139ccfabee55ddb83e7937f5802c0a67aee8975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[michael barnett captured the fire on intersta...</td>\n",
       "      <td>[Country band Lady Antebellum's bus caught fir...</td>\n",
       "      <td>[2.0, 3.0, 2.6666666666666665, 3.3333333333333...</td>\n",
       "      <td>[2.0, 3.0, 2.6666666666666665, 3.3333333333333...</td>\n",
       "      <td>[2.6666666666666665, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[2.3333333333333335, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>(CNN)Lady Antebellum singer Hillary Scott's to...</td>\n",
       "      <td>cnn-test-88c2481234e763c9bbc68d0ab1be1d2375c1349a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[deep reddish color caught seattle native tim ...</td>\n",
       "      <td>[Smoke from massive fires in Siberia created f...</td>\n",
       "      <td>[1.6666666666666667, 3.6666666666666665, 3.333...</td>\n",
       "      <td>[1.6666666666666667, 3.6666666666666665, 1.666...</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 4.666666666666667, 5.0, 5...</td>\n",
       "      <td>[2.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>(CNN)A fiery sunset greeted people in Washingt...</td>\n",
       "      <td>cnn-test-a02e362c5b8f049848ce718b37b96117485461cf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   machine_summaries  \\\n",
       "0  [donald sterling , nba team last year . sterli...   \n",
       "1  [north pacific gray whale has earned a spot in...   \n",
       "2  [russian fighter jet intercepted a u.s. reconn...   \n",
       "3  [michael barnett captured the fire on intersta...   \n",
       "4  [deep reddish color caught seattle native tim ...   \n",
       "\n",
       "                                     human_summaries  \\\n",
       "0  [V. Stiviano must pay back $2.6 million in gif...   \n",
       "1  [The whale, Varvara, swam a round trip from Ru...   \n",
       "2  [The incident occurred on April 7 north of Pol...   \n",
       "3  [Country band Lady Antebellum's bus caught fir...   \n",
       "4  [Smoke from massive fires in Siberia created f...   \n",
       "\n",
       "                                           relevance  \\\n",
       "0  [1.6666666666666667, 1.6666666666666667, 2.333...   \n",
       "1  [2.3333333333333335, 4.666666666666667, 3.6666...   \n",
       "2  [4.0, 4.0, 4.0, 3.3333333333333335, 3.33333333...   \n",
       "3  [2.0, 3.0, 2.6666666666666665, 3.3333333333333...   \n",
       "4  [1.6666666666666667, 3.6666666666666665, 3.333...   \n",
       "\n",
       "                                           coherence  \\\n",
       "0  [1.3333333333333333, 3.0, 1.0, 2.6666666666666...   \n",
       "1  [1.3333333333333333, 4.666666666666667, 3.6666...   \n",
       "2  [3.3333333333333335, 4.333333333333333, 1.6666...   \n",
       "3  [2.0, 3.0, 2.6666666666666665, 3.3333333333333...   \n",
       "4  [1.6666666666666667, 3.6666666666666665, 1.666...   \n",
       "\n",
       "                                             fluency  \\\n",
       "0  [1.0, 4.666666666666667, 4.333333333333333, 4....   \n",
       "1  [1.0, 5.0, 4.666666666666667, 3.66666666666666...   \n",
       "2  [3.6666666666666665, 4.333333333333333, 5.0, 4...   \n",
       "3  [2.6666666666666665, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "4  [5.0, 5.0, 5.0, 5.0, 4.666666666666667, 5.0, 5...   \n",
       "\n",
       "                                         consistency  \\\n",
       "0  [1.0, 2.3333333333333335, 4.666666666666667, 5...   \n",
       "1  [1.3333333333333333, 5.0, 5.0, 4.3333333333333...   \n",
       "2  [5.0, 5.0, 4.666666666666667, 5.0, 5.0, 5.0, 5...   \n",
       "3  [2.3333333333333335, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "4  [2.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  (CNN)Donald Sterling's racist remarks cost him...   \n",
       "1  (CNN)A North Pacific gray whale has earned a s...   \n",
       "2  (CNN)After a Russian fighter jet intercepted a...   \n",
       "3  (CNN)Lady Antebellum singer Hillary Scott's to...   \n",
       "4  (CNN)A fiery sunset greeted people in Washingt...   \n",
       "\n",
       "                                                  id  \n",
       "0  cnn-test-404f859482d47c127868964a9a39d1a7645dd2e9  \n",
       "1  cnn-test-4761dc6d8bdf56b9ada97104113dd1bcf4aed3f1  \n",
       "2  cnn-test-5139ccfabee55ddb83e7937f5802c0a67aee8975  \n",
       "3  cnn-test-88c2481234e763c9bbc68d0ab1be1d2375c1349a  \n",
       "4  cnn-test-a02e362c5b8f049848ce718b37b96117485461cf  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ_eval['test'].to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baccd900",
   "metadata": {},
   "source": [
    "We are only interested in the 'text' collumn from this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31cec5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CNN)Donald Sterling's racist remarks cost him an NBA team last year. But now it's his former female companion who has lost big. A Los Angeles judge has ordered V. Stiviano to pay back more than $2.6 million in gifts after Sterling's wife sued her. In the lawsuit, Rochelle \"Shelly\" Sterling accused Stiviano of targeting extremely wealthy older men. She claimed Donald Sterling used the couple's money to buy Stiviano a Ferrari, two Bentleys and a Range Rover, and that he helped her get a $1.8 million duplex. Who is V. Stiviano? Stiviano countered that there was nothing wrong with Donald Sterling giving her gifts and that she never took advantage of the former Los Angeles Clippers owner, who made much of his fortune in real estate. Shelly Sterling was thrilled with the court decision Tuesday, her lawyer told CNN affiliate KABC. \"This is a victory for the Sterling family in recovering the $2,630,000 that Donald lavished on a conniving mistress,\" attorney Pierce O'Donnell said in a statement. \"It also sets a precedent that the injured spouse can recover damages from the recipient of these ill-begotten gifts.\" Stiviano's gifts from Donald Sterling didn't just include uber-expensive items like luxury cars. According to the Los Angeles Times, the list also includes a $391 Easter bunny costume, a $299 two-speed blender and a $12 lace thong. Donald Sterling's downfall came after an audio recording surfaced of the octogenarian arguing with Stiviano. In the tape, Sterling chastises Stiviano for posting pictures on social media of her posing with African-Americans, including basketball legend Magic Johnson. \"In your lousy f**ing Instagrams, you don't have to have yourself with -- walking with black people,\" Sterling said in the audio first posted by TMZ. He also tells Stiviano not to bring Johnson to Clippers games and not to post photos with the Hall of Famer so Sterling's friends can see. \"Admire him, bring him here, feed him, f**k him, but don't put (Magic) on an Instagram for the world to have to see so they have to call me,\" Sterling said. NBA Commissioner Adam Silver banned Sterling from the league, fined him $2.5 million and pushed through a charge to terminate all of his ownership rights in the franchise. Fact check: Donald Sterling's claims vs. reality CNN's Dottie Evans contributed to this report.\n"
     ]
    }
   ],
   "source": [
    "print(summ_eval['test'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c33fd0",
   "metadata": {},
   "source": [
    "## 🔄 Preparing Data for Evaluation\n",
    "\n",
    "Before running our comparison, we need to convert the dataset to JSONL format and upload it to the Together AI platform.\n",
    "\n",
    "The evaluation service requires:\n",
    "- JSONL format with consistent fields across all examples\n",
    "- Upload with `purpose=\"eval\"` to indicate evaluation usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2fbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading file tmpgi1edogk.jsonl: 100%|██████████| 213k/213k [00:01<00:00, 149kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: id='file-a691355a-07e8-4543-8fca-0630f5a06bee' object=<ObjectType.File: 'file'> created_at=1752955140 type=None purpose=<FilePurpose.Eval: 'eval'> filename='tmpgi1edogk.jsonl' bytes=213087 line_count=0 processed=True FileType='jsonl'\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'text' column to JSONL format and upload for evaluation\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Create a temporary file with JSONL format\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:\n",
    "    for item in summ_eval['test']:\n",
    "        json.dump({'text': item['text']}, f)\n",
    "        f.write('\\n')\n",
    "    temp_file_path = f.name\n",
    "\n",
    "# Upload the file using together_client\n",
    "uploaded_file = together_client.files.upload(\n",
    "    file=temp_file_path,\n",
    "    purpose='eval'\n",
    ")\n",
    "\n",
    "# Clean up the temporary file\n",
    "os.unlink(temp_file_path)\n",
    "\n",
    "print(f\"Uploaded file: {uploaded_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025f47e",
   "metadata": {},
   "source": [
    "## ⚙️ Model Configuration\n",
    "\n",
    "We'll compare two models on the summarization task:\n",
    "- **Model A**: First model for comparison\n",
    "- **Model B**: Second model for comparison  \n",
    "- **Judge Model**: Evaluates which summary is better based on our criteria\n",
    "\n",
    "The judge will assess summaries across multiple dimensions including accuracy, completeness, conciseness, clarity, and relevance.\n",
    "\n",
    "We use the prompts below to setup the models being evaluated and the Judge LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84ad5aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_generation_template = \"\"\"You are an expert summarizer. \n",
    "Your task is to create a concise, accurate summary.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Read the text carefully\n",
    "- Extract the main points and key information\n",
    "- Write 2-3 clear, focused sentences\n",
    "- Prioritize the most important aspects\n",
    "\"\"\"\n",
    "\n",
    "compare_judge_template = \"\"\"You are an expert judge evaluating the quality of text summaries. Your task is to compare two summaries and determine which one is better.\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "1. **Accuracy & Faithfulness**: Does the summary accurately represent the source text without hallucinations or distortions?\n",
    "2. **Completeness**: Does the summary capture all key points and main ideas from the source text?\n",
    "3. **Conciseness**: Is the summary appropriately brief while maintaining essential information?\n",
    "4. **Clarity & Readability**: Is the summary well-written, coherent, and easy to understand?\n",
    "5. **Relevance**: Does the summary focus on the most important aspects of the source text?\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Read the source text carefully\n",
    "- Evaluate both Summary A and Summary B against each criterion\n",
    "- Consider the overall quality and usefulness of each summary\n",
    "- Give a brief explanation (2-3 sentences) justifying your choice\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "880c7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_A_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "MODEL_B_NAME = \"Qwen/Qwen2.5-72B-Instruct-Turbo\"\n",
    "\n",
    "JUDGE_MODEL_NAME = \"deepseek-ai/DeepSeek-V3\"\n",
    "\n",
    "# Model configurations\n",
    "model_a_config = {\n",
    "    \"model_name\": MODEL_A_NAME,\n",
    "    \"system_template\": summarization_generation_template,\n",
    "    \"input_template\": \"{{text}}\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.5\n",
    "}\n",
    "\n",
    "model_b_config = {\n",
    "    \"model_name\": MODEL_B_NAME,\n",
    "    \"system_template\": summarization_generation_template,\n",
    "    \"input_template\": \"{{text}}\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a2de5",
   "metadata": {},
   "source": [
    "## 🏃‍♂️ Running the Comparison Evaluation\n",
    "\n",
    "The `compare` evaluation type performs a comprehensive head-to-head comparison:\n",
    "\n",
    "1. **Two-pass evaluation**: Each model generates responses in different orders to eliminate position bias \n",
    "2. **Judge assessment**: The judge model evaluates both outputs and determines the winner\n",
    "3. **Detailed feedback**: Provides reasoning for each decision\n",
    "\n",
    "**Key Parameters:**\n",
    "- `type`: Set to `\"compare\"` for head-to-head evaluation\n",
    "- `model_a` / `model_b`: Configurations for the two models being compared\n",
    "- `judge_model_name`: The model that will make the comparison decisions\n",
    "- `judge_system_template`: Detailed criteria for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6947a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ID: eval-2f8c-1752678421\n",
      "Status: pending\n"
     ]
    }
   ],
   "source": [
    "# Create compare evaluation\n",
    "evaluation_response = together_client.evaluation.create(\n",
    "    type=\"compare\",\n",
    "    input_data_file_path=uploaded_file.id,\n",
    "    judge_model_name=JUDGE_MODEL_NAME,\n",
    "    judge_system_template=compare_judge_template,\n",
    "    model_a=model_a_config,\n",
    "    model_b=model_b_config\n",
    ")\n",
    "\n",
    "print(f\"Evaluation ID: {evaluation_response.workflow_id}\")\n",
    "print(f\"Status: {evaluation_response.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ee809",
   "metadata": {},
   "source": [
    "## 📊 Understanding Comparison Results\n",
    "\n",
    "Once the evaluation is completed we can examine the results.\n",
    "\n",
    "The evaluation provides several key metrics:\n",
    "- **A_wins**: Number of times Model A was preferred\n",
    "- **B_wins**: Number of times Model B was preferred  \n",
    "- **Ties**: Number of cases where both models performed equally\n",
    "- **Fail counts**: Generation or judge failures (should be 0 for successful runs)\n",
    "\n",
    "\n",
    "Each result includes both the original and flipped evaluations to ensure that position bias from the judge is eliminated:\n",
    "\n",
    "### Two-Pass Evaluation Process\n",
    "1. **First pass**: Model A generates first, then Model B\n",
    "2. **Second pass**: Model B generates first, then Model A\n",
    "\n",
    "Here we see that model B won in 28 cases, model A won in 21 cases, and 51 cases were ties according to our judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5e310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"A_wins\": 21,\n",
      "  \"B_wins\": 28,\n",
      "  \"Ties\": 51,\n",
      "  \"generation_fail_count\": 0,\n",
      "  \"judge_fail_count\": 0,\n",
      "  \"result_file_id\": \"file-e4054d52-a503-4260-893e-7c2b117ba20c\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "status_compare_models = together_client.evaluation.status(evaluation_response.workflow_id)\n",
    "\n",
    "print(json.dumps(status_compare_models.results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e0b9f",
   "metadata": {},
   "source": [
    "## 🔍 Examining Detailed Results\n",
    "\n",
    "Each evaluation result contains:\n",
    "- **Original input**: The text that was summarized\n",
    "- **Model outputs**: Summaries from both Model A and Model B\n",
    "- **Judge decisions**: Both original and flipped evaluation results\n",
    "- **Final decision**: The conclusive winner after bias elimination\n",
    "\n",
    "The `final_decision` field shows the judge's ultimate verdict after considering both evaluation passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d1bb4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file summary_bench_results_a.jsonl: 100%|██████████| 321k/321k [00:00<00:00, 2.94MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FileObject(object='local', id='file-e4054d52-a503-4260-893e-7c2b117ba20c', filename='/Users/zain/Documents/Projects/together-cookbook/Evals/summary_bench_results_a.jsonl', size=321499)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMPARE_MODELS_FILE = \"./summary_bench_results_a.jsonl\"\n",
    "\n",
    "compare_models_file_id = status_compare_models.results['result_file_id']\n",
    "together_client.files.retrieve_content(compare_models_file_id, output=COMPARE_MODELS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b3d089b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1:\n",
      "  text: Liverpool vice-captain Jordan Henderson thinks his side could catch Manchester City in the Barclays Premier League having fought through a tough and long season at Anfield. Henderson and Liverpool goalkeeper Simon Mignolet both played their 47th game of season in the 2-0 win over Newcastle United on Monday night, equalling the record for appearances by any player in the top five European leagues so far this campaign. But the England midfielder believes that after finding winning form again following poor results against Manchester United and Arsenal, Liverpool can pile the pressure on to City who sit four points above them in the race for the Champions League. Liverpool vice-captain Jordan Henderson thinks his side could catch Manchester City in the  Premier League Henderson played his 47th game of season in the 2-0 win over Newcastle United on Monday night Manchester City have been faltering and lost 4-2 at Manchester United, Liverpool are four points behind 'We knew it was an important game for us,' Henderson said 'Obviously the last couple of weeks in the Premier League have been disappointing both with performances and results. We knew today we needed to put that right and I think we did that with a great result for us. 'We've got ground to make up but we'll just keep going, keep winning in games and putting in performances. Come the end of the season, you never know.' Raheem Sterling pulled off some fine skill to open the scoring in the ninth minute at Anfield before Joe Allen capitalised on a defensive mistake to score his first Anfield goal with 20 minutes to go. Raheem Sterling pulled off some fine skill to open the scoring in the ninth minute at Anfield on Monday night Joe Allen capitalised on a defensive mistake to score his first Anfield goal to make it 2-0 with 20 minutes to go 'It was great to get that goal to give us that two-goal cushion,' Allen said. 'It wasn't my best game but I got the goal and helped the team. It was important after the defeats.' Newcastle had what looked a strong shout for a penalty when Dejan Lovren appeared to foul Ayoze Perez in the area, and Allen admitted he thought it was a spot-kick. 'I did get a good view of it as I'd missed the ball in the build-up,' he said. 'It look a good shout for a penalty, but I didn't get one the other night. These things happen.' Liverpool's players congratulate Allen on his goal that secured another vital win in the Premier League battle\n",
      "  MODEL_TO_EVALUATE_OUTPUT_A: Liverpool vice-captain Jordan Henderson believes his team can catch Manchester City in the Premier League, despite being four points behind, after a 2-0 win over Newcastle United. The victory, which equaled the record for top five European league appearances this season, marked a turnaround in form for Liverpool following disappointing results against Manchester United and Arsenal. Henderson stated that his team will continue to win games and put in strong performances to close the gap with Manchester City.\n",
      "  MODEL_TO_EVALUATE_OUTPUT_B: Liverpool vice-captain Jordan Henderson believes his team can catch Manchester City in the Premier League after a 2-0 win over Newcastle United, despite being four points behind. Henderson and goalkeeper Simon Mignolet both played their 47th game of the season, matching a record for appearances in top European leagues. Raheem Sterling and Joe Allen scored the goals, with Allen acknowledging a potential missed penalty call but emphasizing the importance of the win.\n",
      "  choice_original: A\n",
      "  choice_flipped: A\n",
      "  final_decision: A\n",
      "  is_incomplete: False\n",
      "\n",
      "Line 2:\n",
      "  text: (CNN)One of the biggest TV events of all time is being reimagined for new audiences. \"Roots,\" the epic miniseries about an African-American slave and his descendants, had a staggering audience of over 100 million viewers back in 1977. Now A&E networks are remaking the miniseries, to air in 2016. A&E, Lifetime and History (formerly the History Channel) announced Thursday that the three networks would simulcast a remake of the saga of Kunta Kinte, an African who was captured, shipped to America and sold into slavery to work on a Virginia plantation. LeVar Burton, who portrayed Kinte in the original, will co-executive produce the new miniseries. A press release describes the new version as \"original\" and \"contemporary\" and will draw more from Alex Haley's classic novel, \"Roots: The Saga of an American Family.\" Producers will consult scholars in African and African-American history for added authenticity. \"We are proud to bring this saga to fans of the original, as well as to a new generation that will experience this powerful and poignant tale for the first time,\" said Dirk Hoogstra, History's executive vice president and general manager. \"Audiences will once again feel the impact of Kunta Kinte's indomitable spirit.\" Executive producer Mark Wolper, son of the original's producer David L. Wolper, added, \"Kunta Kinte began telling his story over 200 years ago and that story went through his family lineage, to Alex Haley, to my father, and now the mantle rests with me. Like Kunta Kinte fought to tell his story over and over again, so must we.\" The remade \"Roots\" will encounter a new generation of viewers who have witnessed Barack Obama make history as the nation's first African-American president and \"12 Years a Slave\" win the Oscar for Best Picture, but also widespread racial unrest over police treatment of black suspects in many U.S. cities. \"My career began with 'Roots' and I am proud to be a part of this new adaptation,\" said Burton. \"There is a huge audience of contemporary young Americans who do not know the story of 'Roots' or its importance.\"\n",
      "  MODEL_TO_EVALUATE_OUTPUT_A: A&E networks are remaking the iconic miniseries \"Roots,\" which originally aired in 1977 and was watched by over 100 million viewers. The new version, set to air in 2016, will be co-executive produced by LeVar Burton, who portrayed Kunta Kinte in the original, and will draw more from Alex Haley's classic novel. The remake aims to bring the powerful story of an African-American slave and his descendants to a new generation of viewers, with producers consulting scholars for added authenticity.\n",
      "  MODEL_TO_EVALUATE_OUTPUT_B: A&E Networks, Lifetime, and History are collaborating to remake the iconic 1977 miniseries \"Roots,\" which originally drew over 100 million viewers. The new version, set to air in 2016, will be co-executive produced by LeVar Burton and will draw more heavily from Alex Haley's novel, with input from scholars to ensure authenticity. The remake aims to introduce the powerful story of Kunta Kinte and his descendants to a new generation, amid a contemporary context of racial issues and progress.\n",
      "  choice_original: B\n",
      "  choice_flipped: A\n",
      "  final_decision: Tie\n",
      "  is_incomplete: False\n",
      "\n",
      "Line 3:\n",
      "  text: Arsenal playmaker Mesut Ozil seemed to be in a enjoying his home comforts after arriving back in London following the international break, posting a picture putting his feet up with his dog. The German star was in fine form as the World Cup winners beat Georgia 2-0 in their Euro 2016 qualifier on Sunday, and is now setting his sights on the Premier League encounter with Liverpool at the weekend. And Ozil gave an insight into what he does in his downtime after taking a snap of him playing with his pet with the caption: 'Home Sweet Home'. Mesut Ozil posted a picture on Twitter relaxing with his dog on Wednesday after returning home to London Ozil impressed on international duty as Germany beat Gerogia 2-0 in a Euro 2016 qualifier on Sunday The midfielder, who was signed for £42.5million from Real Madrid in 2013, missed Arsenal's last game against Newcastle through illness, but will be expected to start against Liverpool at the Emirates Stadium. Ozil was said to have been spotted in a Berlin nightclub after missing the Gunners' 2-1 win at St James' Park, though manager Arsene Wenger rubbished those claims. He has been doing the business on the pitch after returning from injury though,  wracking up four assists and scoring three goals since January. And the former Schalke man says he is finally adjusting to English football, claiming he fits in 'perfectly'. 'There are a lot of what we call 'English weeks' in Germany - where you play twice, and there's no winter break either,' he said. 'It's demanding because you don't have many breaks to recover, but as a footballer it's great to have so many games and I'm the sort of player who prefers playing in many matches - that's why I fit perfectly here.' The playmaker has been in fine form since returning from injury for Arsenal this year\n",
      "  MODEL_TO_EVALUATE_OUTPUT_A: Mesut Ozil has returned to London following the international break and is preparing for Arsenal's upcoming Premier League encounter with Liverpool. The German star, who impressed in Germany's 2-0 Euro 2016 qualifier win over Georgia, has been enjoying his downtime with his dog and has expressed his satisfaction with the demanding Premier League schedule, stating he fits in \"perfectly\" in English football. Ozil will be expected to start against Liverpool at the Emirates Stadium this weekend.\n",
      "  MODEL_TO_EVALUATE_OUTPUT_B: Mesut Ozil, Arsenal's playmaker, shared a relaxing photo with his dog after returning from international duty, where he helped Germany secure a 2-0 win over Georgia in a Euro 2016 qualifier. Despite missing Arsenal's last match due to illness, Ozil is expected to start against Liverpool this weekend, having recently recovered from injury and contributed significantly with four assists and three goals since January. He also noted his adjustment to the demanding English football schedule, stating that he fits in \"perfectly\" due to the frequent matches.\n",
      "  choice_original: B\n",
      "  choice_flipped: B\n",
      "  final_decision: B\n",
      "  is_incomplete: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print first 3 lines of the comparison results file\n",
    "with open(COMPARE_MODELS_FILE, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 3:\n",
    "            break\n",
    "        print(f\"Line {i+1}:\")\n",
    "        data = json.loads(line.strip())\n",
    "        for key, value in data.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "250887ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL COMPARISON RESULTS ===\n",
      "Total Evaluations: 100\n",
      "Model A - meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo Wins: 21 (21.0%)\n",
      "Model B - Qwen/Qwen2.5-72B-Instruct-Turbo Wins: 28 (28.0%)\n",
      "Ties: 51 (51.0%)\n",
      "\n",
      "🏆 Winner: Model B - Qwen/Qwen2.5-72B-Instruct-Turbo by 7 evaluations\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display final results\n",
    "total_comparisons = status_compare_models.results['A_wins'] + status_compare_models.results['B_wins'] + status_compare_models.results['Ties']\n",
    "a_wins = status_compare_models.results['A_wins']\n",
    "b_wins = status_compare_models.results['B_wins']  \n",
    "ties = status_compare_models.results['Ties']\n",
    "\n",
    "print(\"=== FINAL COMPARISON RESULTS ===\")\n",
    "print(f\"Total Evaluations: {total_comparisons}\")\n",
    "print(f\"Model A - {MODEL_A_NAME} Wins: {a_wins} ({a_wins/total_comparisons*100:.1f}%)\")\n",
    "print(f\"Model B - {MODEL_B_NAME} Wins: {b_wins} ({b_wins/total_comparisons*100:.1f}%)\")\n",
    "print(f\"Ties: {ties} ({ties/total_comparisons*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "if b_wins > a_wins:\n",
    "    winner = f\"Model B - {MODEL_B_NAME}\"\n",
    "    margin = b_wins - a_wins\n",
    "elif a_wins > b_wins:\n",
    "    winner = f\"Model A - {MODEL_A_NAME}\" \n",
    "    margin = a_wins - b_wins\n",
    "else:\n",
    "    winner = \"Tie\"\n",
    "    margin = 0\n",
    "\n",
    "if winner != \"Tie\":\n",
    "    print(f\"🏆 Winner: {winner} by {margin} evaluations\")\n",
    "else:\n",
    "    print(\"🤝 Overall tie between models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a3a8e",
   "metadata": {},
   "source": [
    "## 🔑 Key Findings\n",
    "\n",
    "**Performance Summary:**\n",
    "- **Model B** outperformed Model A with **28 wins vs 21 wins** (7-point advantage)\n",
    "- **High tie rate** of **51%** suggests both models often produce comparable summaries\n",
    "- **No failures** in generation or judging indicates robust model performance\n",
    "\n",
    "**Insights:**\n",
    "- The close competition (28 vs 21) suggests both models have similar summarization capabilities\n",
    "- The high tie percentage (51%) indicates that for many articles, both models produced summaries of equivalent quality\n",
    "- Model B's slight edge may be due to better handling of specific article types or summary characteristics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
