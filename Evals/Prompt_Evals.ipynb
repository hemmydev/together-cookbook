{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b81817",
   "metadata": {},
   "source": [
    "# Prompt Comparison with LLM Judge Evaluation\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/togethercomputer/together-cookbook/blob/main/Evals/Prompt_Evals.ipynb)\n",
    "\n",
    "<img src=\"../images/prompt_compare.png\" width=\"750\">\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to compare different prompts or even model settings(`max_tokens`, `temperature` etc.) for the same model to ensure we are using the optimal setup.\n",
    "\n",
    "Using the Together AI [Evaluations API](https://docs.together.ai/docs/ai-evaluations) we'll:\n",
    "\n",
    "1. Load the SummEval dataset containing news articles to summarize\n",
    "2. Configure two different prompts for the same model\n",
    "3. Use a judge model to evaluate which prompt produces better summaries\n",
    "4. Analyze the prompt optimization results\n",
    "\n",
    "The full list of supported models can be found [here](https://docs.together.ai/docs/evaluations-supported-models).\n",
    "\n",
    "\n",
    "**Concepts Covered:**\n",
    "- **Prompt Engineering**: Comparing simple vs. detailed prompts for the same task\n",
    "- **LLM-as-a-Judge**: Using a language model to evaluate prompt effectiveness  \n",
    "- **Compare Evaluation**: A/B testing prompts to determine optimal configurations\n",
    "- **Summarization Evaluation**: Assessing summary quality across multiple criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c2fc0",
   "metadata": {},
   "source": [
    "# 📋 Prompt Comparison Overview\n",
    "\n",
    "Prompt engineering is crucial for getting optimal performance from language models. This evaluation compares:\n",
    "\n",
    "- **Prompt A (Simple)**: Basic instruction with minimal guidance\n",
    "- **Prompt B (Structured)**: Detailed instruction with specific guidelines  \n",
    "\n",
    "We'll test which approach produces better summaries on news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25580c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup and installation\n",
    "!pip install -qU together datasets pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ed98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import together\n",
    "import pandas as pd\n",
    "\n",
    "# Set your API key: export TOGETHER_API_KEY=\"your-key-here\"\n",
    "# Or set it programmatically: \n",
    "# together_client = together.Client(api_key=\"your-key-here\")\n",
    "together_client = together.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c5635a",
   "metadata": {},
   "source": [
    "## 📊 Understanding the SummEval Dataset\n",
    "\n",
    "The SummEval dataset contains news articles paired with both human and machine-generated summaries, along with quality ratings across multiple dimensions like relevance, coherence, fluency, and consistency.\n",
    "\n",
    "**Dataset Structure:**\n",
    "- `text`: The original news article to be summarized\n",
    "- `machine_summaries`: Various automated summaries\n",
    "- `human_summaries`: Human-written reference summaries\n",
    "- Quality ratings across multiple evaluation criteria\n",
    "\n",
    "For our evaluation, we'll focus on the original articles and generate new summaries using different prompts with our target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f6bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['machine_summaries', 'human_summaries', 'relevance', 'coherence', 'fluency', 'consistency', 'text', 'id'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "summ_eval = load_dataset(\"mteb/summeval\")\n",
    "\n",
    "summ_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fbc9bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machine_summaries</th>\n",
       "      <th>human_summaries</th>\n",
       "      <th>relevance</th>\n",
       "      <th>coherence</th>\n",
       "      <th>fluency</th>\n",
       "      <th>consistency</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[donald sterling , nba team last year . sterli...</td>\n",
       "      <td>[V. Stiviano must pay back $2.6 million in gif...</td>\n",
       "      <td>[1.6666666666666667, 1.6666666666666667, 2.333...</td>\n",
       "      <td>[1.3333333333333333, 3.0, 1.0, 2.6666666666666...</td>\n",
       "      <td>[1.0, 4.666666666666667, 4.333333333333333, 4....</td>\n",
       "      <td>[1.0, 2.3333333333333335, 4.666666666666667, 5...</td>\n",
       "      <td>(CNN)Donald Sterling's racist remarks cost him...</td>\n",
       "      <td>cnn-test-404f859482d47c127868964a9a39d1a7645dd2e9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[north pacific gray whale has earned a spot in...</td>\n",
       "      <td>[The whale, Varvara, swam a round trip from Ru...</td>\n",
       "      <td>[2.3333333333333335, 4.666666666666667, 3.6666...</td>\n",
       "      <td>[1.3333333333333333, 4.666666666666667, 3.6666...</td>\n",
       "      <td>[1.0, 5.0, 4.666666666666667, 3.66666666666666...</td>\n",
       "      <td>[1.3333333333333333, 5.0, 5.0, 4.3333333333333...</td>\n",
       "      <td>(CNN)A North Pacific gray whale has earned a s...</td>\n",
       "      <td>cnn-test-4761dc6d8bdf56b9ada97104113dd1bcf4aed3f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[russian fighter jet intercepted a u.s. reconn...</td>\n",
       "      <td>[The incident occurred on April 7 north of Pol...</td>\n",
       "      <td>[4.0, 4.0, 4.0, 3.3333333333333335, 3.33333333...</td>\n",
       "      <td>[3.3333333333333335, 4.333333333333333, 1.6666...</td>\n",
       "      <td>[3.6666666666666665, 4.333333333333333, 5.0, 4...</td>\n",
       "      <td>[5.0, 5.0, 4.666666666666667, 5.0, 5.0, 5.0, 5...</td>\n",
       "      <td>(CNN)After a Russian fighter jet intercepted a...</td>\n",
       "      <td>cnn-test-5139ccfabee55ddb83e7937f5802c0a67aee8975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[michael barnett captured the fire on intersta...</td>\n",
       "      <td>[Country band Lady Antebellum's bus caught fir...</td>\n",
       "      <td>[2.0, 3.0, 2.6666666666666665, 3.3333333333333...</td>\n",
       "      <td>[2.0, 3.0, 2.6666666666666665, 3.3333333333333...</td>\n",
       "      <td>[2.6666666666666665, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[2.3333333333333335, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>(CNN)Lady Antebellum singer Hillary Scott's to...</td>\n",
       "      <td>cnn-test-88c2481234e763c9bbc68d0ab1be1d2375c1349a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[deep reddish color caught seattle native tim ...</td>\n",
       "      <td>[Smoke from massive fires in Siberia created f...</td>\n",
       "      <td>[1.6666666666666667, 3.6666666666666665, 3.333...</td>\n",
       "      <td>[1.6666666666666667, 3.6666666666666665, 1.666...</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 4.666666666666667, 5.0, 5...</td>\n",
       "      <td>[2.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>(CNN)A fiery sunset greeted people in Washingt...</td>\n",
       "      <td>cnn-test-a02e362c5b8f049848ce718b37b96117485461cf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   machine_summaries  \\\n",
       "0  [donald sterling , nba team last year . sterli...   \n",
       "1  [north pacific gray whale has earned a spot in...   \n",
       "2  [russian fighter jet intercepted a u.s. reconn...   \n",
       "3  [michael barnett captured the fire on intersta...   \n",
       "4  [deep reddish color caught seattle native tim ...   \n",
       "\n",
       "                                     human_summaries  \\\n",
       "0  [V. Stiviano must pay back $2.6 million in gif...   \n",
       "1  [The whale, Varvara, swam a round trip from Ru...   \n",
       "2  [The incident occurred on April 7 north of Pol...   \n",
       "3  [Country band Lady Antebellum's bus caught fir...   \n",
       "4  [Smoke from massive fires in Siberia created f...   \n",
       "\n",
       "                                           relevance  \\\n",
       "0  [1.6666666666666667, 1.6666666666666667, 2.333...   \n",
       "1  [2.3333333333333335, 4.666666666666667, 3.6666...   \n",
       "2  [4.0, 4.0, 4.0, 3.3333333333333335, 3.33333333...   \n",
       "3  [2.0, 3.0, 2.6666666666666665, 3.3333333333333...   \n",
       "4  [1.6666666666666667, 3.6666666666666665, 3.333...   \n",
       "\n",
       "                                           coherence  \\\n",
       "0  [1.3333333333333333, 3.0, 1.0, 2.6666666666666...   \n",
       "1  [1.3333333333333333, 4.666666666666667, 3.6666...   \n",
       "2  [3.3333333333333335, 4.333333333333333, 1.6666...   \n",
       "3  [2.0, 3.0, 2.6666666666666665, 3.3333333333333...   \n",
       "4  [1.6666666666666667, 3.6666666666666665, 1.666...   \n",
       "\n",
       "                                             fluency  \\\n",
       "0  [1.0, 4.666666666666667, 4.333333333333333, 4....   \n",
       "1  [1.0, 5.0, 4.666666666666667, 3.66666666666666...   \n",
       "2  [3.6666666666666665, 4.333333333333333, 5.0, 4...   \n",
       "3  [2.6666666666666665, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "4  [5.0, 5.0, 5.0, 5.0, 4.666666666666667, 5.0, 5...   \n",
       "\n",
       "                                         consistency  \\\n",
       "0  [1.0, 2.3333333333333335, 4.666666666666667, 5...   \n",
       "1  [1.3333333333333333, 5.0, 5.0, 4.3333333333333...   \n",
       "2  [5.0, 5.0, 4.666666666666667, 5.0, 5.0, 5.0, 5...   \n",
       "3  [2.3333333333333335, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "4  [2.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  (CNN)Donald Sterling's racist remarks cost him...   \n",
       "1  (CNN)A North Pacific gray whale has earned a s...   \n",
       "2  (CNN)After a Russian fighter jet intercepted a...   \n",
       "3  (CNN)Lady Antebellum singer Hillary Scott's to...   \n",
       "4  (CNN)A fiery sunset greeted people in Washingt...   \n",
       "\n",
       "                                                  id  \n",
       "0  cnn-test-404f859482d47c127868964a9a39d1a7645dd2e9  \n",
       "1  cnn-test-4761dc6d8bdf56b9ada97104113dd1bcf4aed3f1  \n",
       "2  cnn-test-5139ccfabee55ddb83e7937f5802c0a67aee8975  \n",
       "3  cnn-test-88c2481234e763c9bbc68d0ab1be1d2375c1349a  \n",
       "4  cnn-test-a02e362c5b8f049848ce718b37b96117485461cf  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ_eval['test'].to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0bd62d",
   "metadata": {},
   "source": [
    "We are only interested in the 'text' collumn from this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad2271e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CNN)Donald Sterling's racist remarks cost him an NBA team last year. But now it's his former female companion who has lost big. A Los Angeles judge has ordered V. Stiviano to pay back more than $2.6 million in gifts after Sterling's wife sued her. In the lawsuit, Rochelle \"Shelly\" Sterling accused Stiviano of targeting extremely wealthy older men. She claimed Donald Sterling used the couple's money to buy Stiviano a Ferrari, two Bentleys and a Range Rover, and that he helped her get a $1.8 million duplex. Who is V. Stiviano? Stiviano countered that there was nothing wrong with Donald Sterling giving her gifts and that she never took advantage of the former Los Angeles Clippers owner, who made much of his fortune in real estate. Shelly Sterling was thrilled with the court decision Tuesday, her lawyer told CNN affiliate KABC. \"This is a victory for the Sterling family in recovering the $2,630,000 that Donald lavished on a conniving mistress,\" attorney Pierce O'Donnell said in a statement. \"It also sets a precedent that the injured spouse can recover damages from the recipient of these ill-begotten gifts.\" Stiviano's gifts from Donald Sterling didn't just include uber-expensive items like luxury cars. According to the Los Angeles Times, the list also includes a $391 Easter bunny costume, a $299 two-speed blender and a $12 lace thong. Donald Sterling's downfall came after an audio recording surfaced of the octogenarian arguing with Stiviano. In the tape, Sterling chastises Stiviano for posting pictures on social media of her posing with African-Americans, including basketball legend Magic Johnson. \"In your lousy f**ing Instagrams, you don't have to have yourself with -- walking with black people,\" Sterling said in the audio first posted by TMZ. He also tells Stiviano not to bring Johnson to Clippers games and not to post photos with the Hall of Famer so Sterling's friends can see. \"Admire him, bring him here, feed him, f**k him, but don't put (Magic) on an Instagram for the world to have to see so they have to call me,\" Sterling said. NBA Commissioner Adam Silver banned Sterling from the league, fined him $2.5 million and pushed through a charge to terminate all of his ownership rights in the franchise. Fact check: Donald Sterling's claims vs. reality CNN's Dottie Evans contributed to this report.\n"
     ]
    }
   ],
   "source": [
    "print(summ_eval['test'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef054111",
   "metadata": {},
   "source": [
    "## 🔄 Preparing Data for Evaluation\n",
    "\n",
    "Before running our comparison, we need to convert the dataset to JSONL format and upload it to the Together AI platform.\n",
    "\n",
    "The evaluation service requires:\n",
    "- JSONL format with consistent fields across all examples\n",
    "- Upload with `purpose=\"eval\"` to indicate evaluation usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fad63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading file tmpdpd9nmgt.jsonl: 100%|██████████| 213k/213k [00:00<00:00, 266kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: id='file-06a8ad03-c29e-4550-87ef-51fab736d47e' object=<ObjectType.File: 'file'> created_at=1752957728 type=None purpose=<FilePurpose.Eval: 'eval'> filename='tmpdpd9nmgt.jsonl' bytes=213087 line_count=0 processed=True FileType='jsonl'\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'text' column to JSONL format and upload for evaluation\n",
    "import tempfile\n",
    "import json\n",
    "\n",
    "# Create a temporary file with JSONL format\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:\n",
    "    for item in summ_eval['test']:\n",
    "        json.dump({'text': item['text']}, f)\n",
    "        f.write('\\n')\n",
    "    temp_file_path = f.name\n",
    "\n",
    "# Upload the file using together_client\n",
    "uploaded_file = together_client.files.upload(\n",
    "    file=temp_file_path,\n",
    "    purpose='eval'\n",
    ")\n",
    "\n",
    "# Clean up the temporary file\n",
    "os.unlink(temp_file_path)\n",
    "\n",
    "print(f\"Uploaded file: {uploaded_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420b1b7d",
   "metadata": {},
   "source": [
    "## ⚙️ Prompt Configurations\n",
    "\n",
    "We'll compare the same model but using two different prompts:\n",
    "- **Prompt A**: An overly simplistic prompt for comparison\n",
    "- **Prompt B**: A well structured second prompt for comparison - *in this toy evaluation we expect this prompt to win* \n",
    "- **Judge Model**: Evaluates which summary is better based on our criteria\n",
    "\n",
    "We use the prompts below to setup the models being evaluated and the Judge LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99745b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_judge_template = \"\"\"You are an expert judge evaluating the quality of text summaries. Your task is to compare two summaries and determine which one is better.\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "1. **Accuracy & Faithfulness**: Does the summary accurately represent the source text without hallucinations or distortions?\n",
    "2. **Completeness**: Does the summary capture all key points and main ideas from the source text?\n",
    "3. **Conciseness**: Is the summary appropriately brief while maintaining essential information?\n",
    "4. **Clarity & Readability**: Is the summary well-written, coherent, and easy to understand?\n",
    "5. **Relevance**: Does the summary focus on the most important aspects of the source text?\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Read the source text carefully\n",
    "- Evaluate both Summary A and Summary B against each criterion\n",
    "- Consider the overall quality and usefulness of each summary\n",
    "- Give a brief explanation (2-3 sentences) justifying your choice\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65f41c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_prompt_A = \"\"\"You are an expert summarizer. \n",
    "Your task is to create a concise, accurate summary.\n",
    "\"\"\"\n",
    "\n",
    "summarization_prompt_B = \"\"\"You are an expert summarizer. \n",
    "Your task is to create a concise, accurate summary.\n",
    "\n",
    "Please follow these guidelines when creating your summary:\n",
    "1. Read the entire text carefully to understand the main points\n",
    "2. Identify the key themes, arguments, and conclusions\n",
    "3. Write a summary that is approximately 25% of the original length\n",
    "4. Use clear, concise language and maintain the original tone\n",
    "5. Include the most important facts, figures, and examples\n",
    "6. Ensure the summary flows logically from one point to the next\n",
    "7. Avoid adding your own opinions or interpretations\n",
    "8. Focus on the author's main message and supporting evidence\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231e53ee",
   "metadata": {},
   "source": [
    "## 🏃‍♂️ Running the Prompt Comparison\n",
    "\n",
    "The evaluation will:\n",
    "1. Generate summaries using both prompts on the same model\n",
    "2. Have a judge model compare the quality of outputs\n",
    "3. Determine which prompt performs better overall\n",
    "\n",
    "**Key Parameters:**\n",
    "- Same base model for both configurations\n",
    "- Different system prompts (simple vs. structured)\n",
    "- Judge evaluates based on accuracy, completeness, and clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c0713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ID: eval-8af6-1752679553\n",
      "Status: pending\n"
     ]
    }
   ],
   "source": [
    "# Model configurations\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-72B-Instruct-Turbo\"\n",
    "\n",
    "JUDGE_MODEL_NAME = \"deepseek-ai/DeepSeek-V3\"\n",
    "\n",
    "model_a_config = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"system_template\": summarization_prompt_A,\n",
    "    \"input_template\": \"{{text}}\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.5\n",
    "}\n",
    "\n",
    "model_b_config = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"system_template\": summarization_prompt_B,\n",
    "    \"input_template\": \"{{text}}\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.5\n",
    "}\n",
    "\n",
    "# Create compare evaluation\n",
    "evaluation_response = together_client.evaluation.create(\n",
    "    type=\"compare\",\n",
    "    input_data_file_path=uploaded_file.id,\n",
    "    judge_model_name=JUDGE_MODEL_NAME,\n",
    "    judge_system_template=compare_judge_template,\n",
    "    model_a=model_a_config,\n",
    "    model_b=model_b_config\n",
    ")\n",
    "\n",
    "print(f\"Evaluation ID: {evaluation_response.workflow_id}\")\n",
    "print(f\"Status: {evaluation_response.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f980b",
   "metadata": {},
   "source": [
    "## 📊 Understanding the Results\n",
    "\n",
    "Once the evaluation is done running we can see results.\n",
    "\n",
    "The comparison provides key metrics:\n",
    "- **A_wins**: Times the simple prompt was preferred  \n",
    "- **B_wins**: Times the structured prompt was preferred\n",
    "- **Ties**: Cases where both prompts performed equally\n",
    "- **Fail counts**: Generation errors (should be 0)\n",
    "\n",
    "A clear winner indicates one prompting approach is more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7cd9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"A_wins\": 6,\n",
      "  \"B_wins\": 30,\n",
      "  \"Ties\": 64,\n",
      "  \"generation_fail_count\": 0,\n",
      "  \"judge_fail_count\": 0,\n",
      "  \"result_file_id\": \"file-30124ed1-a78b-4a82-968a-09bcbcf1ec09\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "status_compare_prompts = together_client.evaluation.status(evaluation_response.workflow_id)\n",
    "\n",
    "print(json.dumps(status_compare_prompts.results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f40871a",
   "metadata": {},
   "source": [
    "## 🔑 Key Findings\n",
    "\n",
    "**Prompt Optimization Results:**\n",
    "- **Structured Prompt (B)** significantly outperformed Simple Prompt (A): **30 wins vs 6 wins**\n",
    "- **High tie rate** of **64%** suggests many cases where prompt differences were minimal\n",
    "- **Clear winner**: Structured prompts with detailed guidelines produce better summaries\n",
    "\n",
    "**Insights:**\n",
    "- Detailed instructions (8 specific guidelines) dramatically improve summary quality\n",
    "- The 5:1 win ratio (30 vs 6) shows structured prompts are much more effective\n",
    "- 64% ties indicate both approaches often produce acceptable results, but structured prompts excel in edge cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
