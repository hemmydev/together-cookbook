{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serial Chain Agent Workflow\n",
    "Author: [Zain Hasan](https://x.com/ZainHasan6)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/togethercomputer/together-cookbook/blob/main/Agents/Serial_Chain_Agent_Workflow.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we'll create an LLM agent workflow that will produce an audio podcast file using the contents of a provided PDF.\n",
    "\n",
    "In this **serial chain agent workflow**, we will call multiple LLMs consecutively (hence the name serial chain), where future LLM calls consume outputs from previous calls. The input to our workflow will be the raw extracted PDF text, and the output will be a JSON object that stores the lines our podcast host and guest will say.\n",
    "Our serial chain will include the following steps:\n",
    "\n",
    "1. LLM Call 1: Clean and extract details given source text extracted from a PDF\n",
    "2. LLM Call 2: Generate an outline given extracted information and the source text\n",
    "3. LLM Call 3: Generate a script given the information from step 1 and outline from step 2. This script will be a structured JSON object\n",
    "4. Text-to-Speech Model Call: Text-to-Speech model to generate the podcast using the script\n",
    "\n",
    "Before we implement the PDF to podcast workflow, let's first understand how simple the serial chain agent workflow is to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serial Prompt Chain Agent Workflow\n",
    "\n",
    "The serial prompt chain agent workflow takes the input prompt and processes it with consecutive LLM calls, feeding the output of the current LLM call to the next one down the chain. Optionally, you can also have a check between every consecutive LLM call that determines if we should break out of the chain earlier and return the response or terminate the workflow entirely.\n",
    "\n",
    "The diagram below details the workflow:\n",
    "\n",
    "<img src=\"../images/serial_chain.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see this workflow in code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# For MAC OS X\n",
    "#!brew install ffmpeg\n",
    "# For Linux\n",
    "!apt install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install libraries\n",
    "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n",
    "!pip install -qU pydantic together pypdf ffmpeg-python cartesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import together\n",
    "from together import Together\n",
    "\n",
    "from typing import Any, Optional, Dict, List, Literal\n",
    "from pydantic import Field, BaseModel, ValidationError\n",
    "\n",
    "TOGETHER_API_KEY = \"--Your API Key--\"\n",
    "\n",
    "client = Together(api_key= TOGETHER_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LLM call helper function\n",
    "def run_llm(user_prompt : str, model : str, system_prompt : Optional[str] = None):\n",
    "    \"\"\" Run the language model with the given user prompt and system prompt. \"\"\"\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=4000,        \n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Simple JSON mode LLM call helper function\n",
    "def JSON_llm(user_prompt : str, schema : BaseModel, system_prompt : Optional[str] = None):\n",
    "    \"\"\" Run a language model with the given user prompt and system prompt, and return a structured JSON object. \"\"\"\n",
    "    try:\n",
    "        messages = []\n",
    "        if system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "        \n",
    "        extract = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "            response_format={\n",
    "                \"type\": \"json_object\",\n",
    "                \"schema\": schema.model_json_schema(),\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        response = json.loads(extract.choices[0].message.content)\n",
    "        return response\n",
    "        \n",
    "    except ValidationError as e:\n",
    "        raise ValueError(f\"Schema validation failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serial_chain_workflow(input_query: str, prompt_chain : List[str]) -> List[str]:\n",
    "    \"\"\"Run a serial chain of LLM calls to address the `input_query` \n",
    "    using a prompts specified in a list `prompt_chain`.\n",
    "\n",
    "    Outputs the chain of responses from the LLM models.\n",
    "    \"\"\"\n",
    "    response_chain = [] # Will store the responses from the LLM models\n",
    "    response = input_query\n",
    "    for i, prompt in enumerate(prompt_chain):\n",
    "        print(f\"STEP {i+1}\\n\")\n",
    "        response = run_llm(f\"{prompt}\\nInput:\\n{response}\", model='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo')\n",
    "        response_chain.append(response)\n",
    "        print(f\"{response}\\n\")\n",
    "    return response_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1\n",
      "\n",
      "$12 (hourly wage), \n",
      "50 minutes (time worked)\n",
      "\n",
      "To find the earnings, we need to convert 50 minutes to hours and multiply it by the hourly wage.\n",
      "\n",
      "STEP 2\n",
      "\n",
      "1. Convert 50 minutes to hours by dividing by 60 (since 1 hour = 60 minutes).\n",
      "   50 minutes / 60 = 0.83 hours\n",
      "\n",
      "2. Multiply the converted hours by the hourly wage.\n",
      "   0.83 hours * $12/hour = earnings\n",
      "\n",
      "STEP 3\n",
      "\n",
      "To find the earnings, we need to multiply the converted hours by the hourly wage.\n",
      "\n",
      "0.83 hours * $12/hour = earnings\n",
      "earnings = 0.83 * 12\n",
      "earnings = $9.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Toy example run of prompt chain agent workflow\n",
    "question = \"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\"\n",
    "\n",
    "prompt_chain = [\"\"\"Given the math problem, ONLY extract any relevant numerical information and how it can be used.\"\"\",\n",
    "                \"\"\"Given the numberical information extracted, ONLY express the steps you would take to solve the problem.\"\"\",\n",
    "                \"\"\"Given the steps, express the final answer to the problem.\"\"\"]\n",
    "\n",
    "responses = serial_chain_workflow(question, prompt_chain)\n",
    "\n",
    "final_answer = responses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podcast Generation Using Prompt Chain Agent Workflow\n",
    "\n",
    "Now that we've solved a toy example using the prompt chaining agent workflow lets implement a bespoke version of this same workflow to handle a more complicated task: generation of an audio podcast file using the contents of a PDF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The system prompt will be the same for all LLMs in the chain\n",
    "SYSTEM_PROMPT = \"\"\"You are an experienced world-class podcast producer tasked with transforming the provided \n",
    "input text into an engaging and informative podcast.\n",
    "\n",
    "You are to follow a step by step methodical process to generate the final podcast which involves:\n",
    "1. Reading and extracting relevant information and snippets from the source document.\n",
    "2. Using the relevant information compiled in step 1, creating an outline document containing brainstormed ideas, summarized topics that should be covered, questions and how to guide the conversation \n",
    "3. Using the details from step 1 and 2 you then need to put together a script for the podcast.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a class that will control the structure that our script will be generated with.\n",
    "\n",
    "We want the script to be a list of `DiologueItem`'s(a single line) + the speaker for that line. We also want to give the LLM throwaway token in the form of a scratch-pad so that it can generate better quality lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueItem(BaseModel):\n",
    "    \"\"\"A single dialogue item.\"\"\"\n",
    "\n",
    "    speaker: Literal[\"Host (Jane)\", \"Guest\"]\n",
    "    text: str\n",
    "\n",
    "\n",
    "class Dialogue(BaseModel):\n",
    "    \"\"\"The dialogue between the host and guest.\"\"\"\n",
    "\n",
    "    scratchpad: str\n",
    "    name_of_guest: str\n",
    "    dialogue: List[DialogueItem]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF Content Extraction\n",
    "\n",
    "The input to our agent workflow will be content extracted from a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-30 13:02:50--  https://arxiv.org/pdf/2406.04692\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.195.42, 151.101.67.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1157463 (1.1M) [application/pdf]\n",
      "Saving to: ‘2406.04692’\n",
      "\n",
      "2406.04692          100%[===================>]   1.10M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2025-03-30 13:02:50 (24.2 MB/s) - ‘2406.04692’ saved [1157463/1157463]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://arxiv.org/pdf/2406.04692\n",
    "!mv 2406.04692 MoA.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pathlib\n",
    "from pathlib import Path\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def get_PDF_text(file : str):\n",
    "    text = ''\n",
    "\n",
    "    # Read the PDF file and extract text\n",
    "    try:\n",
    "        with Path(file).open(\"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            text = \"\\n\\n\".join([page.extract_text() for page in reader.pages])\n",
    "    except Exception as e:\n",
    "        raise f\"Error reading the PDF file: {str(e)}\"\n",
    "\n",
    "        # Check if the PDF has more than ~131,072 characters\n",
    "        # The context lenght limit of the model is 131,072 tokens and thus the text should be less than this limit\n",
    "    if len(text) > 400_000:\n",
    "        raise \"The PDF is too long. Please upload a PDF with fewer than ~131072 characters.\"\n",
    "\n",
    "    return text\n",
    "\n",
    "text = get_PDF_text('./MoA.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets chain each LLM call\n",
    "\n",
    "We will run each LLM in series and examine the intermediate outputs - this is a good idea when first setting up the agent workflow so that you can optimize and improve each step.\n",
    "Steps:\n",
    "\n",
    "1. LLM Call 1: Clean and extract details given source text extracted from a PDF\n",
    "2. LLM Call 2: Generate an outline given extracted information and the source text\n",
    "3. LLM Call 3: Generate a script given the information from step 1 and outline from step 2. This script will be a structured JSON object\n",
    "4. Text-to-Speech Model Call: Text-to-Speech model to generate the podcast using the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extract details from the source document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Extracting Relevant Information and Snippets from the Source Document\n",
      "\n",
      "The provided source document discusses a novel approach to enhancing the capabilities of large language models (LLMs) by leveraging the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. Key points and insights extracted from the document include:\n",
      "\n",
      "1. **Introduction to MoA**: The MoA approach is designed to harness the strengths of multiple LLMs to improve their reasoning and language generation capabilities. It involves constructing a layered architecture where each layer comprises multiple LLM agents, with each agent taking the outputs from agents in the previous layer as auxiliary information to generate its response.\n",
      "\n",
      "2. **Collaborativeness of LLMs**: The document highlights the phenomenon of collaborativeness among LLMs, where models tend to generate better responses when provided with outputs from other models, even if those outputs are of lower quality. This phenomenon is crucial for the MoA methodology.\n",
      "\n",
      "3. **Mixture-of-Agents Methodology**: The MoA structure is illustrated, showing how LLMs in each layer process inputs and generate outputs based on the outputs from the previous layer. The methodology does not require fine-tuning and utilizes the prompting and generation capabilities of off-the-shelf LLMs.\n",
      "\n",
      "4. **Evaluation and Results**: Comprehensive evaluations were conducted on benchmarks such as AlpacaEval 2.0, MT-Bench, and FLASK. The results demonstrate significant improvements with the MoA approach, achieving state-of-the-art performance on these benchmarks. Notably, the MoA method using only open-source models outperformed GPT-4 Omni on AlpacaEval 2.0.\n",
      "\n",
      "5. **Analysis of MoA Components**: Experiments were conducted to understand the internal mechanism of MoA, including the effects of the number of proposer models, the role of different models as proposers vs. aggregators, and the impact of model diversity. The results show that MoA significantly outperforms LLM rankers and tends to incorporate the best proposed answers.\n",
      "\n",
      "6. **Cost Effectiveness and Latency**: An analysis of the cost-effectiveness and latency of the MoA approach versus other models like GPT-4 Turbo and GPT-4o was performed. The results indicate that MoA can achieve higher performance while being more cost-effective and having comparable latency.\n",
      "\n",
      "7. **Related Work and Conclusion**: The document discusses related work in LLM reasoning and model ensemble techniques, concluding that the MoA approach offers a novel and effective way to leverage the strengths of multiple LLMs for improved performance.\n",
      "\n",
      "8. **Limitations and Future Work**: The potential limitation of high Time to First Token (TTFT) due to iterative aggregation is noted, suggesting future work could explore chunk-wise aggregation to mitigate this issue.\n",
      "\n",
      "### Key Snippets for Podcast Discussion\n",
      "\n",
      "- **MoA Methodology**: \"Our approach, Mixture-of-Agents, leverages the collective strengths of multiple large language models to iteratively enhance generation quality.\"\n",
      "- **Collaborativeness**: \"The collaborativeness phenomenon among LLMs, where models generate better responses with outputs from other models, is key to MoA's success.\"\n",
      "- **Evaluation Results**: \"MoA achieves state-of-the-art performance on AlpacaEval 2.0, MT-Bench, and FLASK, outperforming GPT-4 Omni on AlpacaEval 2.0 with only open-source models.\"\n",
      "- **Cost-Effectiveness**: \"MoA is not only high-performing but also cost-effective compared to models like GPT-4 Turbo, making it a viable option for applications requiring both quality and efficiency.\"\n",
      "\n",
      "These snippets and points provide a solid foundation for a podcast discussion about the MoA approach, its implications for LLM development, and its potential applications in AI and natural language processing.\n"
     ]
    }
   ],
   "source": [
    "# Since we want every LLM on the chain to execute slightly different tasks, we will define different user prompts for each LLM\n",
    "\n",
    "CLEAN_EXTRACT_DETAILS = \"\"\"The first step you need to perform is to extract details from the source document that are informative\n",
    "and listeners will find useful to understand the source document better.\n",
    "\n",
    "The input may be unstructured or messy, sourced from PDFs or web pages. \n",
    "\n",
    "Your goal is to extract the most interesting and insightful content for a compelling podcast discussion.\n",
    "\n",
    "Source Document: {source_doc}\n",
    "\"\"\"\n",
    "\n",
    "source_doc = text\n",
    "\n",
    "extracted_details = run_llm(CLEAN_EXTRACT_DETAILS.format(source_doc=source_doc), \n",
    "                            model='meta-llama/Llama-3.3-70B-Instruct-Turbo', \n",
    "                            system_prompt=SYSTEM_PROMPT)\n",
    "print(extracted_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate an outline based on the extracted details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Podcast Outline: Exploring the Mixture-of-Agents (MoA) Methodology for Enhancing Large Language Models (LLMs)\n",
      "\n",
      "#### Introduction (5 minutes)\n",
      "- **Brief Overview of LLMs**: Introduce the concept of Large Language Models, their current capabilities, and limitations.\n",
      "- **Introduction to MoA**: Explain the Mixture-of-Agents methodology, its purpose, and how it leverages the strengths of multiple LLMs.\n",
      "\n",
      "#### Segment 1: Understanding MoA (15 minutes)\n",
      "- **Collaborativeness of LLMs**: Discuss the phenomenon where LLMs generate better responses when provided with outputs from other models.\n",
      "- **MoA Structure**: Describe the layered architecture of MoA, how each layer processes inputs, and generates outputs based on previous layer outputs.\n",
      "- **Key Components**: Highlight the importance of proposer and aggregator models within the MoA framework.\n",
      "\n",
      "#### Segment 2: Evaluation and Results (15 minutes)\n",
      "- **Benchmark Performance**: Discuss the performance of MoA on benchmarks like AlpacaEval 2.0, MT-Bench, and FLASK.\n",
      "- **Comparison with State-of-the-Art Models**: Compare MoA's performance with models like GPT-4 Omni, highlighting its achievements and any limitations.\n",
      "- **Cost-Effectiveness and Latency**: Analyze the cost-effectiveness and latency of MoA compared to other models, emphasizing its viability for practical applications.\n",
      "\n",
      "#### Segment 3: Internal Mechanism and Future Directions (15 minutes)\n",
      "- **What Makes MoA Work**: Explore the experiments conducted to understand MoA's internal mechanism, including the effects of the number of proposer models and model diversity.\n",
      "- **Specialization of Models**: Discuss how different models excel in specific roles within the MoA ecosystem.\n",
      "- **Future Work and Limitations**: Address the potential limitations of MoA, such as high Time to First Token (TTFT), and suggest future directions, including chunk-wise aggregation.\n",
      "\n",
      "#### Segment 4: Related Work and Broader Impact (10 minutes)\n",
      "- **LLM Reasoning and Model Ensemble Techniques**: Briefly overview related work in LLM reasoning and model ensemble techniques.\n",
      "- **Broader Impact**: Discuss how MoA could enhance the effectiveness of LLM-driven applications, improve model interpretability, and make AI more accessible.\n",
      "\n",
      "#### Conclusion (5 minutes)\n",
      "- **Summary of Key Points**: Recap the main points discussed about the MoA methodology.\n",
      "- **Final Thoughts and Future Perspectives**: Offer final thoughts on the potential of MoA to revolutionize LLM capabilities and invite listeners to consider the implications of such advancements in AI and natural language processing.\n",
      "\n",
      "#### Brainstormed Ideas for Engagement:\n",
      "- **Hypothetical Scenario**: Present a scenario where MoA is applied to solve a complex problem, demonstrating its potential real-world impact.\n",
      "- **Expert Interview**: Invite an expert in AI or NLP to discuss the implications and future directions of MoA.\n",
      "- **Audience Engagement**: Encourage listeners to share their thoughts or questions about MoA and its potential applications through social media or a dedicated forum.\n",
      "- **Simplified Analogies**: Use analogies to explain complex concepts, such as comparing the MoA methodology to a team of experts working together to solve a puzzle.\n",
      "- **Interactive Elements**: Incorporate interactive elements, such as quizzes or polls, to keep the audience engaged and interested in the topic.\n"
     ]
    }
   ],
   "source": [
    "OUTLINE_PROMPT = \"\"\"The second step is to use the extracted information from the source document to write an outline and brainstorm ideas.\n",
    "\n",
    "The source document and extracted details are provided below:\n",
    "\n",
    "Extracted Details: {extracted_details}\n",
    "\n",
    "Source Document: {source_doc}\n",
    "\n",
    "Steps to follow when generating an outline and brainstorming ideas for the discussion in the podcast:\n",
    "\n",
    "1. Analyze the Input:\n",
    "   Carefully examine the extracted details in the text above, identifying key topics, points, and \n",
    "   interesting facts or anecdotes that could drive an engaging podcast conversation. \n",
    "   Disregard irrelevant information.\n",
    "\n",
    "2. Brainstorm Ideas:\n",
    "   Creatively brainstorm ways to present the key points engagingly. \n",
    "   \n",
    "   Consider:\n",
    "   - Analogies, storytelling techniques, or hypothetical scenarios to make content relatable\n",
    "   - Ways to make complex topics accessible to a general audience\n",
    "   - Thought-provoking questions to explore during the podcast\n",
    "   - Creative approaches to fill any gaps in the information\n",
    "   - Make sure that all important details extracted above are covered in the outline that you draft\n",
    "\"\"\"\n",
    "\n",
    "outline = run_llm(OUTLINE_PROMPT.format(extracted_details=extracted_details, source_doc=source_doc),\n",
    "                    model='meta-llama/Llama-3.3-70B-Instruct-Turbo', \n",
    "                    system_prompt=SYSTEM_PROMPT)\n",
    "\n",
    "print(outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate a script.\n",
    "\n",
    "Below we define a class that will control the structure that our script will be generated with.\n",
    "\n",
    "We want the script to be a list of `DialogueItems` (a single line) + the speaker for that line. We also want to give the LLM a throwaway token in the form of a scratch pad so that it can generate better quality lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueItem(BaseModel):\n",
    "    \"\"\"A single dialogue item.\"\"\"\n",
    "\n",
    "    speaker: Literal[\"Host (Jane)\", \"Guest\"]\n",
    "    text: str\n",
    "\n",
    "\n",
    "class Dialogue(BaseModel):\n",
    "    \"\"\"The dialogue between the host and guest.\"\"\"\n",
    "\n",
    "    scratchpad: str\n",
    "    name_of_guest: str\n",
    "    dialogue: List[DialogueItem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scratchpad': \"Let's create a compelling podcast script about the Mixture-of-Agents (MoA) methodology for enhancing Large Language Models (LLMs).\",\n",
       " 'name_of_guest': 'Dr. Maria Hernandez, AI Researcher',\n",
       " 'dialogue': [{'speaker': 'Host (Jane)',\n",
       "   'text': \"Welcome to today's episode of 'AI Explained'! I'm your host, Jane. Joining me is Dr. Maria Hernandez, an expert in AI research. Dr. Hernandez, thanks for being here!\"},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': \"Thanks, Jane! I'm excited to discuss the Mixture-of-Agents methodology and its potential to revolutionize Large Language Models.\"},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': \"Let's dive right in. Can you explain what Large Language Models are and their current limitations?\"},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': 'Large Language Models, or LLMs, are AI systems that can process and generate human-like language. However, they have limitations, such as struggling with complex reasoning and generating coherent text.'},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': \"That's fascinating. So, how does the Mixture-of-Agents methodology address these limitations?\"},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': \"MoA leverages the collective strengths of multiple LLMs to improve their reasoning and language generation capabilities. It's like having a team of experts working together to solve a puzzle.\"},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': 'I love that analogy. Can you walk us through the MoA structure and how it works?'},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': 'MoA consists of multiple layers, each comprising multiple LLM agents. Each agent takes the outputs from previous agents as auxiliary information to generate its response.'},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': 'That sounds like a complex process. How does MoA achieve state-of-the-art performance on benchmarks like AlpacaEval 2.0 and MT-Bench?'},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': \"MoA's success can be attributed to the collaborativeness among LLMs. When provided with outputs from other models, LLMs generate better responses. This phenomenon is key to MoA's performance.\"},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': \"Wow, that's impressive. What about cost-effectiveness and latency? How does MoA compare to other models like GPT-4 Turbo?\"},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': 'MoA is not only high-performing but also cost-effective compared to models like GPT-4 Turbo. It achieves comparable latency while being more efficient.'},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': \"That's great news for practical applications. What are some potential limitations of MoA, and how can they be addressed?\"},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': 'One potential limitation is high Time to First Token (TTFT) due to iterative aggregation. Future work could explore chunk-wise aggregation to mitigate this issue.'},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': \"Thanks for sharing your insights, Dr. Hernandez. Before we wrap up, what's the broader impact of MoA on the future of LLMs and AI?\"},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': \"MoA has the potential to revolutionize LLM capabilities, making them more effective, interpretable, and accessible. It's an exciting time for AI research!\"},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': 'Thanks for joining us today, Dr. Hernandez. To our listeners, thanks for tuning in. Join the conversation on social media using the hashtag #AIExplained.'}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a JSON structured script based on the extracted details and the outline\n",
    "\n",
    "SCRIPT_PROMPT = \"\"\"The last step is to use the extracted details and the ideas brainstormed in the outline below to craft\n",
    "a script for the podcast.\n",
    "\n",
    "Extracted Details: {extracted_details}\n",
    "\n",
    "Using the outline provided here: {outline}\n",
    "\n",
    "Steps to follow when generating the script:\n",
    "\n",
    " 1. **Craft the Dialogue:**\n",
    "   Develop a natural, conversational flow between the host (Jane) and the guest speaker (the author or an expert on the topic).\n",
    "   In the `<scratchpad>`, creatively brainstorm ways to present the key points engagingly.\n",
    "   \n",
    "   Incorporate:\n",
    "   - The best ideas from your brainstorming session\n",
    "   - Clear explanations of complex topics\n",
    "   - An engaging and lively tone to captivate listeners\n",
    "   - A balance of information and entertainment\n",
    "\n",
    "   Rules for the dialogue:\n",
    "   - The host (Jane) always initiates the conversation and interviews the guest\n",
    "   - Include thoughtful questions from the host to guide the discussion\n",
    "   - Incorporate natural speech patterns, including occasional verbal fillers (e.g., \"Uhh\", \"Hmmm\", \"um,\" \"well,\" \"you know\")\n",
    "   - Allow for natural interruptions and back-and-forth between host and guest - this is very important to make the conversation feel authentic\n",
    "   - Ensure the guest's responses are substantiated by the input text, avoiding unsupported claims\n",
    "   - Maintain a PG-rated conversation appropriate for all audiences\n",
    "   - Avoid any marketing or self-promotional content from the guest\n",
    "   - The host concludes the conversation\n",
    "\n",
    "2. **Summarize Key Insights:**\n",
    "   Naturally weave a summary of key points into the closing part of the dialogue. This should feel like a casual conversation rather than a formal recap, reinforcing the main takeaways before signing off.\n",
    "\n",
    "3. **Maintain Authenticity:**\n",
    "   Throughout the script, strive for authenticity in the conversation. Include:\n",
    "   - Moments of genuine curiosity or surprise from the host\n",
    "   - Instances where the guest might briefly struggle to articulate a complex idea\n",
    "   - Light-hearted moments or humor when appropriate\n",
    "   - Brief personal anecdotes or examples that relate to the topic (within the bounds of the input text)\n",
    "\n",
    "4. **Consider Pacing and Structure:**\n",
    "   Ensure the dialogue has a natural ebb and flow:\n",
    "   - Start with a strong hook to grab the listener's attention\n",
    "   - Gradually build complexity as the conversation progresses\n",
    "   - Include brief \"breather\" moments for listeners to absorb complex information\n",
    "   - For complicated concepts, reasking similar questions framed from a different perspective is recommended\n",
    "   - End on a high note, perhaps with a thought-provoking question or a call-to-action for listeners\n",
    "\n",
    "IMPORTANT RULE: Each line of dialogue should be no more than 300 characters (e.g., can finish within 30 seconds)\n",
    "\n",
    "Remember: Always reply in valid JSON format, without code blocks. Begin directly with the JSON output.\n",
    "\"\"\"\n",
    "\n",
    "script = JSON_llm(SCRIPT_PROMPT.format(extracted_details=extracted_details, outline=outline),\n",
    "                    Dialogue,\n",
    "                    system_prompt=SYSTEM_PROMPT)\n",
    "\n",
    "script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also define a function that encapsulates the entire workflow for generating a podcast script from a PDF file.\n",
    "def prompt_chain_podcast_workflow(file : str):\n",
    "    text = get_PDF_text(file)\n",
    "    source_doc = text\n",
    "    \n",
    "    extracted_details = run_llm(CLEAN_EXTRACT_DETAILS.format(source_doc=source_doc), \n",
    "                            model='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo', \n",
    "                            system_prompt=SYSTEM_PROMPT)\n",
    "    \n",
    "    outline = run_llm(OUTLINE_PROMPT.format(extracted_details=extracted_details, source_doc=source_doc),\n",
    "                    model='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo',\n",
    "                    system_prompt=SYSTEM_PROMPT)\n",
    "    \n",
    "    script = JSON_llm(SCRIPT_PROMPT.format(extracted_details=extracted_details, outline=outline),\n",
    "                    Dialogue,\n",
    "                    system_prompt=SYSTEM_PROMPT)\n",
    "    return script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Generate audio podcast\n",
    "\n",
    "We can loop through the lines in the script and generate them by calling the TTS model with specific voice and line configurations. The lines are all appended to the same buffer, and once the script finishes, we write this out to a WAV file, ready to be played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def generate_audio(text: str, voice: str):\n",
    "    \"\"\"Generate audio from text using the specified voice model.\"\"\"\n",
    "    url = \"https://api.together.ai/v1/audio/generations\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"input\": text,\n",
    "        \"voice\": voice,\n",
    "        \"response_format\": \"raw\",\n",
    "        \"response_encoding\": \"pcm_f32le\",\n",
    "        \"sample_rate\": 44100,\n",
    "        \"stream\": False,\n",
    "        \"model\": \"cartesia/sonic\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 9c33b2f Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/home/roy/miniconda3/envs/pdf2podcast --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/pkg-config\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "  libpostproc    55.  7.100 / 55.  7.100\n",
      "[f32le @ 0x558509102f40] Estimating duration from bitrate, this may be inaccurate\n",
      "Guessed Channel Layout for Input Stream #0.0 : mono\n",
      "Input #0, f32le, from 'podcast.pcm':\n",
      "  Duration: 00:02:05.49, bitrate: 1411 kb/s\n",
      "    Stream #0:0: Audio: pcm_f32le, 44100 Hz, mono, flt, 1411 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_f32le (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'podcast.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf58.45.100\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, mono, s16, 705 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.91.100 pcm_s16le\n",
      "size=   10809kB time=00:02:05.49 bitrate= 705.6kbits/s speed=3.11e+03x    \n",
      "video:0kB audio:10809kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000705%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ffmpeg\n",
    "\n",
    "host_id = \"laidback woman\" # Jane - host\n",
    "guest_id = \"customer support man\" # Guest\n",
    "\n",
    "with open(\"podcast.pcm\", \"wb\") as f:\n",
    "    for line in script['dialogue']:\n",
    "        if line['speaker'] == \"Guest\":\n",
    "            voice_id = guest_id\n",
    "        else:\n",
    "            voice_id = host_id\n",
    "\n",
    "        #print(f\"{voice_id}: {line['text']}\")\n",
    "        raw_audio = generate_audio('-' + line['text'], voice_id) # the \"-\"\" is to add a pause between speakers\n",
    "\n",
    "        f.write(raw_audio)\n",
    "\n",
    "# Convert the raw PCM bytes to a WAV file.\n",
    "ffmpeg.input(\"podcast.pcm\", format=\"f32le\").output(\"podcast.wav\").run()\n",
    "print(\"Podcast generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(\"podcast.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf2podcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
